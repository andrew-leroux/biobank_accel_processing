---
title: "Processing the 5-second UKB Accelerometry Data"
author: "Andrew Leroux"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    number_sections: true
bibliography: ref.bib
vignette: >
  %\VignetteIndexEntry{Organizing the 5-second UKB Accelerometry Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=TRUE)
```

# Organizing the 5-second Level UKB Accelerometry Data

This vignette is intended to illustrate how to load, merge, and structure the 5-second level UK Biobank (UKB) accelerometry data in the 1440+ 
format suggested by @Leroux2019 and then how to perform some basic analyses and derive some standard features of physical activity. 

To begin, we first load a set of **R** packages for reading in data and data manipulation. 
With the exception of the **lubridate** package used for manipulating date-time objects, the packages we use here are all associated with the *tidyverse*. In addition, we need to set the data path where the 5-second level data files are stored 

NOTE: If processing the UKB accelerometry data locally, all code is self contained with the exception of setting the data path. Once you've changed this one line of code, all the remaining R code should run locally without any further changes. Importantly, this code assumes that the only files you have in this directory are 5-second level UKB accelerometry data. 

```{r packages, message=FALSE}
## If packages are not already installed, first install them and then 
## load all requisite packages
pckgs <- c("readr","tidyr","dplyr","lubridate","stringr")
sapply(pckgs, function(x) if(!require(x,character.only=TRUE,quietly=TRUE)) {
  install.packages(x)
  require(x, character.only=TRUE)
})
## Source an R script which will create a data data path for 
## where all the 5-second level data are stored
## We sore this path as a character vector called "data_path_5sec"
## This is the only line of code you should need to change to run locally
source(file.path("..","..","code","make_data_directories.R"))
## Obtain file names for all files in the directory
data_5_sec_ls <- list.files(data_path_5sec)
```

The object *data_5_sec_ls* is a vector of file names.  naming convention for these files follows the following format:

"subject id"_90004_0_0.csv.gz

These files are compressed csv files. Note that Biobank uses different subject identifiers for each individual application. Thus, no two groups will have identical subject ids. For our application, the first 5 files can be seen below

```{r 5_ids,}
data_5_sec_ls[1:5]
```


## Individual Data File Structure


The 5-second level data are stored as separate zipped .csv files for each participant. Each subjects' data file contains a header with information on the start and end times/dates of recording and two columns with 5-second acceleration (modified ENMO) and imputation indicators. Importantly, the 5-second level data has already been adjusted for daylight savings time crossovers. Specifically, the UK Biobank team removed 1 hour of data (1:00AM-2:00AM) on days coinciding with the end of daylight savings, and imputed 1 hour of missing data (1:00AM-2:00AM) on days coinciding with the start of daylight savings. 



```{r example_file}
accel_1 <- read.table(file.path(data_path_5sec, data_5_sec_ls[1]), header=TRUE, sep=",")
str(accel_1)
summary(accel_1)
```

At a high level, the 
For a detailed description of data pre-processing steps performed by the UKB as well as the UKB imputation procedure, please see @ukb_accel.




## Transforming from long to wide (1440+) format

Encoding time and sampling rate information in column names makes sese for individual files. 
However, if we wish to load all subjects' data simultanesouly, this requires us to either (1) add columns corresponding to subject identifiers and observation times; or (2) store each subjects' data as distinct objects. 
Option (1) increases the size of the data by a non-trivial margin and option (2) requires merging and possibly tranforming the data whenever working with summaries accross multiple subjects' data. Therefore, we follow the data storage protocol proposed in @Leroux2019 whereby we store the accelerometry data in wide format and have two separte data matrices for acceleration and imputation indicators. 


The basic procedure for transforming and merging the data involves looping over subjects, loading each subjects' data which is stored in long format, summarizing at the desired frequency (in our case 1 minute), transforming the subjects' data to wide format, then merging row-wise. To be precise, let $i = 1,\ldots, N$ denote subject, $j=1,\ldots,J_i$ denote day, $t=1,\ldots,1440$ denote minute, and $k=1,\ldots,k_{ijt}$ denote 5-second interval within a minute where $k_{ijt} \in \{1,\ldots,12\}$, indicating the possibility for minutes where not all 5-second intervals are observed (this occurs at the end of the observation period). For every participant we then have $A_{ijtk_{ijt}}$ and $I_{ijtk_{ijt}}$ corresponding to acceleration and imputation data for each subject, day, minute, and 5-second interval during the observation period. We summarize the data as:
$$
\begin{aligned}
    A_{ijt} &= k_{ijt}^{-1}\sum_{k=1}^{k_{ijt}}A_{ijtk} \\ 
    NI_{ijt} &= \sum_{k=1}^{k_{ijt}}(1-I_{ijtk}) \;.
\end{aligned}
$$
Note that we summarize the imputation indicator ($NI_{ijt}$) as the sum of *non-imputed* minutes to again account for the fact that there are minutes which are only partially observed. Summarizing the acceleration data in this way potentially averages imputed and non-imputed data, though after applying an exclusion criteria based on few imputed minutes for a given day the effect of this averaging on resulting analyses is likely limited. Alternative summarization approaches could be considered, including averaging over only non-imputed 5-second intervals. 

Once the data has been aggregated at the minute level, we then transform the data to wide format with one column for subject id, a second column for date, and 1440 columns corresponding to each minute of the day separately for acceleration and imputation data. For most participants this results in 2 separate $8 \times 1442$ matrices since the 5-second data runs from 10AM on day 1 to 10AM on day 8. Data are then merged by combining each of these matrices row-wise, resulting in two acceleration and imputation matrices containing all subjects' minute level data.


## Transforming from long to wide (1440+) format: single subject


We illustrate the transforkmation from long to wide data first using one individuals' data. To transform an individuals' data from long to wide format, we use regular expressions to parse the first column name to get sampling rate, start, and stop times. Then we transform the data to wide form where each row represents one subject-day 12AM-12AM. Note that because of the very large number of subjects with accelerometry data in the UK Biobank, working with the 1440+ format for all subjects using their 5-second level data is computationally (RAM) infeasible. Thus, we aggregate the data at the minute level for all subjects, though this could be changed via a slight modification of the code below. 

```{r transform_single_1440}
## set up a vector of unique minutes 
## this will be used to ensure we have appropriate number of columns
## for all participants when we transform from long to wide
utimes <- sprintf("%02d:%02d", rep(0:23, each=60), rep(0:59, 24))

## get the first participants id
eid <- str_extract(data_5_sec_ls, "[0-9]+")[1]

## read in subject 1's data
accel_1 <- read.table(file.path(data_path_5sec, data_5_sec_ls[1]), header=TRUE, sep=",")

## get portion of the column names which indicates start and stop dates/times
raw_date_1 <- str_extract_all(names(accel_1)[1], "\\.{3,3}([0-9]+\\.){5,5}[0-9]+")[[1]]
## extract start and stop dates/times
date_1     <- str_replace_all(raw_date_1, "\\.{3,3}","") %>% 
                    str_replace_all("\\.","-") %>%
                    str_replace_all("([0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})","\\1 \\2:\\3:\\4")
## create a vector of date-times corresponding to observed data
## note that we set the time zone to be UTC as the 5-second level data has already had 
## time-zone information accounted for
date_time_vec     <- seq(ymd_hms(date_1[1], tz="UTC"),ymd_hms(date_1[2], tz="UTC"), by="5 secs")
date_time_vec_chr <- as.POSIXlt(date_time_vec, tz="UTC")
  
## separate out the date and time components of the date/time vector
time_vec <- sprintf("%02d:%02d", hour(date_time_vec), minute(date_time_vec))
time_vec <- factor(time_vec, levels = utimes)
date_vec <- sprintf("%04d-%02d-%02d", year(date_time_vec), month(date_time_vec), day(date_time_vec))

## combine aceleration and date data
adf_1  <- data.frame("eid"=eid[1],accel_1, time_vec, date_vec, stringsAsFactors = FALSE)
## rename the acceleration column 
colnames(adf_1)[2] <- c("acceleration")
## aggregate data to the minute level
adf_1 <- adf_1 %>% 
             ## group at the minute level
             group_by(date_vec, time_vec) %>%
             ## obtain mean and number of not imputed minutes
             summarize("acceleration" = mean(acceleration,na.rm=TRUE),
                      "imputed" = sum(1-imputed,na.rm=TRUE),
                      "eid" = eid[1]) %>%
             ungroup()
  
## transform to wide format for accelration and imputation separately using tidyr::spread
accel_mat_1  <- adf_1 %>% select(-imputed) %>% spread(time_vec,acceleration)
impute_mat_1 <- adf_1 %>% select(-acceleration) %>% spread(time_vec,imputed)
```

This results in two data matrices of equal dimensions corresponding to an individual's average acceleration (modified ENMO) in mili gravitational units and the total number of *non* imputed 5-second intervals within each minute as can be seen below. The number of columns for all participants will be 1442 where the first two columns are date of wear and subject eid and the remaining columns are acceleration/imputation indicators for the 1440 minutes of the day (12AM-12AM). We print the structure of the first 12 columns of both the acceleration and imputation matrices for this one subject below (12:00-12:10AM across all 8 calendar days). 

A few key obervations from the imputation matrix: 

* The first day (first row) of data has missing data (NA) in the first few columns. This is because the data start at 10AM on the first day of recording
* The second day (second row)  of data has 0 imputed data during 12:00AM-12:10AM as indicated by the value "12" in 3-12 columns of the imputation matrix 
* The third day (third row)  of data has all imputed data during 12:00AM-12:10AM as indicated by the value "0" in 3-12 columns of the imputation matrix 

We conclude here by noting that the way the UK Biobank team imputed acceleration data was to take the average acceleration at that time of day using only non-imputed (estimated wear) data. As a result, you can see that 12:00-12:08AM on the third, fourth, and fifth days report identical values in the acceleration matrix (columns 3-10) and that these values are the average of the second, sixth, seventh, and eigth days' data (again, the first day is missing). 


```{r explore_1_subj}
print(str(accel_mat_1[,1:12]))
print(str(impute_mat_1[,1:12]))
```


## Transforming to wide (1440+) format: all subjects

We can wrap the code above in a for loop to transform all subject's data into wide format. 
Then, we combine all subjects data acceleration and imputation data, separately, into two large matrices. 
The code below will perform this transformation and merging, however it will take quite a long time to run due to the large number of subjects in the UK Biobank. This code is easily parallelized to run on either multiple cores, or submitted as a batch job on a computing cluster. Once the data have been transformed and merged, we would then save the merged data to avoid having to repeatedly re-process the data as this would be unnecessarily time consuming.

```{r tranform_all_1440, eval=FALSE}
## get file names for all 5-second level files in the directory
data_5_sec_ls <- list.files(data_path_5sec)
## total number of subejcts to loop over 
nid <- length(data_5_sec_ls)
## extract all subject ids
eid <- str_extract(data_5_sec_ls, "[0-9]+")

## create vector of names for wide-format data 
accel_mat_nms  <- paste0("accel_mat_",1:nid)
impute_mat_nms <- paste0("impute_mat_",1:nid)

## loop over subjects
for(i in 1:nid){
  
  ## read in subject i's data
  accel_i <- read.table(file.path(data_path_5sec, data_5_sec_ls[i]), header=TRUE, sep=",")
  
  ## verify the sampling rate for the data is 5 seconds
  (interval_i <- str_replace(names(accel_i)[1], "(.*sampleRate\\.+)([0-9]+\\.[aA-zZ]+)","\\2"))
  if(interval_i != "5.seconds") next
  
  ## get portion of the column names which indicates start and stop dates/times
  raw_date_i <- str_extract_all(names(accel_i)[1], "\\.{3,3}([0-9]+\\.){5,5}[0-9]+")[[1]]
  ## extract start and stop dates/times
  date_i     <- str_replace_all(raw_date_i, "\\.{3,3}","") %>% 
                      str_replace_all("\\.","-") %>%
                      str_replace_all("([0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})","\\1 \\2:\\3:\\4")
  ## create a vector of date-times corresponding to observed data
  ## note that we set the time zone to be UTC as the 5-second level data has already had 
  ## time-zone information accounted for
  date_time_vec     <- seq(ymd_hms(date_i[1], tz="UTC"),ymd_hms(date_i[2], tz="UTC"), by="5 secs")
  date_time_vec_chr <- as.POSIXlt(date_time_vec, tz="UTC")
  
  ## separate out the date and time components of the date/time vector
  time_vec <- sprintf("%02d:%02d", hour(date_time_vec), minute(date_time_vec))
  time_vec <- factor(time_vec, levels = utimes)
  date_vec <- sprintf("%04d-%02d-%02d",year(date_time_vec), month(date_time_vec),day(date_time_vec))

  ## combine aceleration and date data
  adf_i  <- data.frame("eid"=eid[i], accel_i, time_vec, date_vec, stringsAsFactors = FALSE)
  ## change column name of acceleration column
  colnames(adf_i)[2] <- "acceleration"
  ## aggregate data to the minute level
  adf_i <- adf_i %>% 
             ## group at the minute level
             group_by(date_vec, time_vec) %>%
             ## obtain mean and number of not imputed minutes
             summarize("acceleration" = mean(acceleration,na.rm=TRUE),
                      "imputed" = sum(1-imputed,na.rm=TRUE),
                      "eid" = eid[1]) %>%
             ungroup()

  ## transform to wide format for accelration and imputation separately using tidyr::spread
  accel_mat_i  <- adf_i %>% select(-imputed) %>% spread(time_vec,acceleration)
  impute_mat_i <- adf_i %>% select(-acceleration) %>% spread(time_vec,imputed)
  
  
  ## assign subject i's data to the corresponding names 
  assign(accel_mat_nms[i], accel_mat_i)
  assign(impute_mat_nms[i], impute_mat_i)
 
  ## clear up the workspace a bit
  rm(accel_i, interval_i, raw_date_i, date_i, 
     date_time_vec,date_time_vec_chr,time_vec,date_vec,
     adf_i,accel_mat_i, impute_mat_i)

}

## combine the data
accel_mat_nms  <- ls()[grepl("accel_mat_[0-9]+", ls())]
impute_mat_nms <- ls()[grepl("impute_mat_[0-9]+", ls())]

eval(parse(text=paste0("accel_mat <- bind_rows(", paste0(accel_mat_nms, collapse=","),")")))
eval(parse(text=paste0("impute_mat <- bind_rows(", paste0(impute_mat_nms, collapse=","),")")))
```


```{r load_processed_data, echo=FALSE}
## load the processed data since we're not going to run the entire reporocessing in this vignette
```



# Analysis pipeline using the transformed data

Our proposed analysis pipeline involves three essential steps:

1. Identify "good" accelerometry data
2. Propose a method for handling imputed and/or missing data
3. Calcuate features of interest (features could be scalar summaries, the full daily activity profiles, or something in-between)
4. Associate these features with on or more variables of interest

Steps 1-3 involve decisions specific to analyzing accelerometry data and the choices invovled are non-trivial. 
In the interest of reproducibility and ease thereof, we propose a fairly straightforward method for identifying "good" accelerometry data which uses only the 5-second level data and UKB provided summary data. Below we present ssteps 1-3 of our pipeline, though we note that there are certainly other ways of addressing steps 1-3 above which are appropriate for a given analysis.  


## Identify "good" accelerometry data

Identifying "good" accelerometry data typically has two major components. 
The first component is to determine whether the data are reliable (i.e. was the device appropriately calibrated). 
Once the a particular devices' data has been deemed "reliable", one then identifies participant complinace with weartime protocols. 

### Identify "good" accelerometry data: reliable data

Here, we opt for a conservative definition of "reliable" data based on variables derived from the UK Biobank team. Specifically, we use three UK Biobank derived variables which relate to accelerometry data quality which are described in the [data showcase](http://biobank.ndph.ox.ac.uk/showcase/) on the UK Biobank website. These varibales are: 

1. [Data field 90015](http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=90015): Did the sbject have at least 72 hours in each one hour period of the 24-hour cycle (across multiple days)
2. [Data field 90016](http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=90016): Was the data well calbirated?
3. [Data field 90017](http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=90017): Was the device calibrated using a subjects' own data?

An individual is determined to have reliable data if the answer to each of the three above questions is yes. Because these variables are not included 
in the 5-second level data, we do not have them in the transformed data frames. As such we need to merge them from a different data source. 
The R script "make_data_directories.R" referenced at the beginning of this vignette contains a path where these data are stored (character vector "data_path_accel_quality_flags") and the name of the file (character vector "data_accel_quality_flags"). We provide some code which load,s merges, and subsets the data based on these criteria below. 


```{r identify_good_accel_data_reliable}
## read in data with data quality flags
xdf     <- read_rds(file.path(data_path_accel_quality_flags, data_accel_quality_flags))
# subset to just those individuals with "good" quality data as measured by UKB
xdf <- xdf %>% 
  mutate(
    ## create an indicator of whether accelerometry data was deeemed "good" by UKB team
    "good_accel_data_ukb" = data_quality_good_wear_time %in% "Yes" &
                            data_quality_good_calibration %in% "Yes" & 
                            data_quality_calibrated_on_own_data %in% "Yes"
      ) %>% 
    filter(good_accel_data_ukb == TRUE)
# number of rows is the number of participants who met these data quality criteria
dim(xdf)
```

Two important notes. First, the data we load with data quality flags has already removed participants who have withdrawn consent to participant in the study. This is a critical part of any data analysis pipeline using the UK Biobank data. Second, any participants who did not complete the accelerometry portion of the study will have missing values for these data quality variables and will thus be filtered out using the code above. 


### Identify "good" accelerometry data: weartime protocol

Regarding weartime protocol, the UK Biobank accelerometry study had a 24-hour wear time protocol. That is, participants were instructed to wear the devices continuously. We determiend a "compliant" (i.e. good) calendar day of data as one where the participant was estimated to be wearing the device at least 95\% of the day (1368 minutes). Estimated wear time is calculated using the 1440+ imputation data matrix as imputation was done whenever the UK Biobank team determined an individual was likely not wearing the device. Thus, imputation indicators act as a reproducible proxy for estimated wear/non-wear time. We note using these imputation indicators suggests a very high degree of compliance with weartime protocols relative to other studies with 24 hour weartime protocols. This may suggest an overly optimistic wear time assessment, though we do not pursue this further at the moment.


```{r identify_good_accel_data_weartime}
## re-name acceleration columns to be easier to work with in matrix/data frame operations
accel_cols <- paste0("MIN",1:1440)
colnames(accel_mat) <- colnames(impute_mat) <- c("date","eid",accel_cols)

## calcualate percent of observed day data which are good, then define a "good" 
## day as those with less than 5% imputed data
impute_mat$pct_good_obs <- rowSums(impute_mat[,utimes], na.rm=TRUE)/(1440*12)
impute_mat$good_day     <- as.numeric(impute_mat$pct_good_obs >= 0.95)
```





## Propose a method for handling imputed and/or missing data



## Exploratory plots of daily activity profiles


```{r plot_PA_profiles, echo=TRUE, message=FALSE}


```


## Calculating scalar summaries of activity profiles


```{r calculate_scalar_summaries, echo=TRUE, message=FALSE}


```



# References
