---
title: "Processing the 5-second UKB Accelerometry Data"
author: "Andrew Leroux"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    number_sections: true
bibliography: ref.bib
vignette: >
  %\VignetteIndexEntry{Organizing the 5-second UKB Accelerometry Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

# Organizing the 5-second Level UKB Accelerometry Data

This vignette is intended to illustrate how to load, merge, and structure the 5-second level UK Biobank (UKB) accelerometry data in the 1440+ 
format suggested by @Leroux2019 and then how to perform some basic analyses and derive some standard features of physical activity. 

To begin, we first load a set of **R** packages for reading in data and data manipulation. 
With the exception of the **lubridate** package used for manipulating date-time objects, the packages we use here are all associated with the *tidyverse*. In addition, we need to set the data path where the 5-second level data files are stored 

NOTE: If processing the UKB accelerometry data locally, all code is self contained with the exception of setting the data path. Once you've changed this one line of code, all the remaining R code should run locally without any further changes. Importantly, this code assumes that the only files you have in this directory are 5-second level UKB accelerometry data. 

```{r packages, echo=TRUE, message=FALSE, eval=TRUE}
## If packages are not already installed, first install them and then 
## load all requisite packages
pckgs <- c("readr","tidyr","dplyr","lubridate","stringr")
sapply(pckgs, function(x) if(!require(x,character.only=TRUE,quietly=TRUE)) {
  install.packages(x)
  require(x, character.only=TRUE)
})
## Source an R script which will create a data data path for 
## where all the 5-second level data are stored
## We sore this path as a character vector called "data_path_5sec"
## This is the only line of code you should need to change to run locally
source(file.path("..","..","code","make_data_directories.R"))
## Obtain file names for all files in the directory
data_5_sec_ls <- list.files(data_path_5sec)
```

The object *data_5_sec_ls* is a vector of file names.  naming convention for these files follows the following format:

"subject id"_90004_0_0.csv.gz

These files are compressed csv files. Note that Biobank uses different subject identifiers for each individual application. Thus, no two groups will have identical subject ids. For our application, the first 5 files can be seen below

```{r 5_ids, echo=TRUE, message=FALSE, eval=TRUE}
data_5_sec_ls[1:5]
```


## Individual Data File Structure


The 5-second level data are stored as separate zipped .csv files for each participant. Each subjects' data file contains a header with information on the start and end times/dates of recording and two columns with 5-second acceleration (modified ENMO) and imputation indicators. Importantly, the 5-second level data has already been adjusted for daylight savings time crossovers. Specifically, the UK Biobank team removed 1 hour of data (1:00AM-2:00AM) on days coinciding with the end of daylight savings, and imputed 1 hour of missing data (1:00AM-2:00AM) on days coinciding with the start of daylight savings. 



```{r example_file, echo=TRUE, message=FALSE, eval=TRUE}
accel_1 <- read.table(file.path(data_path_5sec, data_5_sec_ls[1]), header=TRUE, sep=",")
str(accel_1)
summary(accel_1)
```

At a high level, the 
For a detailed description of data pre-processing steps performed by the UKB as well as the UKB imputation procedure, please see @ukb_accel.




## Transforming from long to wide (1440+) format

Encoding time and sampling rate information in column names makes sese for individual files. 
However, if we wish to load all subjects' data simultanesouly, this requires us to either (1) add columns corresponding to subject identifiers and observation times; or (2) store each subjects' data as distinct objects. 
Option (1) increases the size of the data by a non-trivial margin and option (2) requires merging and possibly tranforming the data whenever working with summaries accross multiple subjects' data. Therefore, we follow the data storage protocol proposed in @Leroux2019 whereby we store the accelerometry data in wide format and have two separte data matrices for acceleration and imputation indicators. 


The basic procedure for transforming and merging the data involves looping over subjects, loading each subjects' data which is stored in long format, summarizing at the desired frequency (in our case 1 minute), transforming the subjects' data to wide format, then merging row-wise. To be precise, let $i = 1,\ldots, N$ denote subject, $j=1,\ldots,J_i$ denote day, $t=1,\ldots,1440$ denote minute, and $k=1,\ldots,k_{ijt}$ denote 5-second interval within a minute where $k_{ijt} \in \{1,\ldots,12\}$, indicating the possibility for minutes where not all 5-second intervals are observed (this occurs at the end of the observation period). For every participant we then have $A_{ijtk_{ijt}}$ and $I_{ijtk_{ijt}}$ corresponding to acceleration and imputation data for each subject, day, minute, and 5-second interval during the observation period. We summarize the data as:
$$
\begin{aligned}
    A_{ijt} &= k_{ijt}^{-1}\sum_{k=1}^{k_{ijt}}A_{ijtk} \\ 
    NI_{ijt} &= \sum_{k=1}^{k_{ijt}}(1-I_{ijtk}) \;.
\end{aligned}
$$
Note that we summarize the imputation indicator ($NI_{ijt}$) as the sum of *non-imputed* minutes to again account for the fact that there are minutes which are only partially observed. Summarizing the acceleration data in this way potentially averages imputed and non-imputed data, though after applying an exclusion criteria based on few imputed minutes for a given day the effect of this averaging on resulting analyses is likely limited. Alternative summarization approaches could be considered, including averaging over only non-imputed 5-second intervals. 

Once the data has been aggregated at the minute level, we then transform the data to wide format with one column for subject id, a second column for date, and 1440 columns corresponding to each minute of the day separately for acceleration and imputation data. For most participants this results in 2 separate $8 \times 1442$ matrices since the 5-second data runs from 10AM on day 1 to 10AM on day 8. Data are then merged by combining each of these matrices row-wise, resulting in two acceleration and imputation matrices containing all subjects' minute level data.


## Transforming from long to wide (1440+) format: single subject


We illustrate the transforkmation from long to wide data first using one individuals' data. To transform an individuals' data from long to wide format, we use regular expressions to parse the first column name to get sampling rate, start, and stop times. Then we transform the data to wide form where each row represents one subject-day 12AM-12AM. Note that because of the very large number of subjects with accelerometry data in the UK Biobank, working with the 1440+ format for all subjects using their 5-second level data is computationally (RAM) infeasible. Thus, we aggregate the data at the minute level for all subjects, though this could be changed via a slight modification of the code below. 

```{r tranform_single_1440, echo=TRUE, message=FALSE, eval=TRUE}
## set up a vector of unique minutes 
## this will be used to ensure we have appropriate number of columns
## for all participants when we transform from long to wide
utimes <- sprintf("%02d:%02d", rep(0:23, each=60), rep(0:59, 24))

## get the first participants id
eid <- str_extract(data_5_sec_ls, "[0-9]+")[1]

## read in subject 1's data
accel_1 <- read.table(file.path(data_path_5sec, data_5_sec_ls[1]), header=TRUE, sep=",")

## get portion of the column names which indicates start and stop dates/times
raw_date_1 <- str_extract_all(names(accel_1)[1], "\\.{3,3}([0-9]+\\.){5,5}[0-9]+")[[1]]
## extract start and stop dates/times
date_1     <- str_replace_all(raw_date_1, "\\.{3,3}","") %>% 
                    str_replace_all("\\.","-") %>%
                    str_replace_all("([0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})","\\1 \\2:\\3:\\4")
## create a vector of date-times corresponding to observed data
## note that we set the time zone to be UTC as the 5-second level data has already had 
## time-zone information accounted for
date_time_vec     <- seq(ymd_hms(date_1[1], tz="UTC"),ymd_hms(date_1[2], tz="UTC"), by="5 secs")
date_time_vec_chr <- as.POSIXlt(date_time_vec, tz="UTC")
  
## separate out the date and time components of the date/time vector
time_vec <- sprintf("%02d:%02d", hour(date_time_vec), minute(date_time_vec))
time_vec <- factor(time_vec, levels = utimes)
date_vec <- sprintf("%04d-%02d-%02d", year(date_time_vec), month(date_time_vec), day(date_time_vec))

## combine aceleration and date data
adf_1  <- data.frame("eid"=eid[1],accel_1, time_vec, date_vec, stringsAsFactors = FALSE)
## rename the acceleration column 
colnames(adf_1)[2] <- c("acceleration")
## aggregate data to the minute level
adf_1 <- adf_1 %>% 
             ## group at the minute level
             group_by(date_vec, time_vec) %>%
             ## obtain mean and number of not imputed minutes
             summarize("acceleration" = mean(acceleration,na.rm=TRUE),
                      "imputed" = sum(1-imputed,na.rm=TRUE),
                      "eid" = eid[1]) %>%
             ungroup()
  
## transform to wide format for accelration and imputation separately using tidyr::spread
accel_mat_1  <- adf_1 %>% select(-imputed) %>% spread(time_vec,acceleration)
impute_mat_1 <- adf_1 %>% select(-acceleration) %>% spread(time_vec,imputed)
```

This results in two data matrices of equal dimensions corresponding to an individual's average acceleration (modified ENMO) in mili gravitational units and the total number of *non* imputed 5-second intervals within each minute as can be seen below. The number of columns for all participants will be 1442 where the first two columns are date of wear and subject eid and the remaining columns are acceleration/imputation indicators for the 1440 minutes of the day (12AM-12AM). We print the structure of the first 10 columns of both the acceleration and imputation matrices for this one subject below. 

A few key obervations from the imputation matrix: 
* The first day (first row) of data has missing data (NA) in the first few columns. This is because the data start at 10AM on the first day of recording
* The second day (second row)  of data has 0 imputed data during 12:00AM-12:10AM as indicated by the value "12" in 3-12 columns of the imputation matrix 
* The third day (third row)  of data has all imputed data during 12:00AM-12:10AM as indicated by the value "0" in 3-12 columns of the imputation matrix 

We conclude here by noting that the way the UK Biobank team imputed acceleration data was to take the average acceleration at that time of day using only non-imputed (estimated wear) data. As a result, you can see that 12:00-12:08AM on the third, fourth, and fifth days report identical values in the acceleration matrix (columns 3-10) and that these values are the average of the second, sixth, seventh, and eigth days' data (again, the first day is missing). 


```{r explore_1_subj, eval=TRUE}
print(str(accel_mat_1[,1:10]))
print(str(impute_mat_1[,1:10]))
```


## Transforming to wide (1440+) format: all subjects

We can wrap the code above in a for loop to transform all subject's data into wide format. 
Then, we combine all subjects data acceleration and imputation data, separately, into two large matrices. 
The code below will perform this transformation and merging, however it will take quite a long time to run due to the large number of subjects in the UK Biobank. This code is easily parallelized to run on either multiple cores, or submitted as a batch job on a computing cluster. 

```{r tranform_all_1440, echo=TRUE, message=FALSE}
## get file names for all 5-second level files in the directory
data_5_sec_ls <- list.files(data_path_5sec)
## total number of subejcts to loop over 
nid <- length(data_5_sec_ls)
## extract all subject ids
eid <- str_extract(data_5_sec_ls, "[0-9]+")

## create vector of names for wide-format data 
accel_mat_nms  <- paste0("accel_mat_",1:nid)
impute_mat_nms <- paste0("impute_mat_",1:nid)

## loop over subjects
for(i in 1:nid){
  
  ## read in subject i's data
  accel_i <- read.table(file.path(data_path_5sec, data_5_sec_ls[i]), header=TRUE, sep=",")
  
  ## verify the sampling rate for the data is 5 seconds
  (interval_i <- str_replace(names(accel_i)[1], "(.*sampleRate\\.+)([0-9]+\\.[aA-zZ]+)","\\2"))
  if(interval_i != "5.seconds") next
  
  ## get portion of the column names which indicates start and stop dates/times
  raw_date_i <- str_extract_all(names(accel_i)[1], "\\.{3,3}([0-9]+\\.){5,5}[0-9]+")[[1]]
  ## extract start and stop dates/times
  date_i     <- str_replace_all(raw_date_i, "\\.{3,3}","") %>% 
                      str_replace_all("\\.","-") %>%
                      str_replace_all("([0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})","\\1 \\2:\\3:\\4")
  ## create a vector of date-times corresponding to observed data
  ## note that we set the time zone to be UTC as the 5-second level data has already had 
  ## time-zone information accounted for
  date_time_vec     <- seq(ymd_hms(date_i[1], tz="UTC"),ymd_hms(date_i[2], tz="UTC"), by="5 secs")
  date_time_vec_chr <- as.POSIXlt(date_time_vec, tz="UTC")
  
  ## separate out the date and time components of the date/time vector
  time_vec <- sprintf("%02d:%02d", hour(date_time_vec), minute(date_time_vec))
  time_vec <- factor(time_vec, levels = utimes)
  date_vec <- sprintf("%04d-%02d-%02d",year(date_time_vec), month(date_time_vec),day(date_time_vec))

  ## combine aceleration and date data
  adf_i  <- data.frame("eid"=eid[i], accel_i, time_vec, date_vec, stringsAsFactors = FALSE)
  ## change column name of acceleration column
  colnames(adf_i)[2] <- "acceleration"
  ## aggregate data to the minute level
  adf_i <- adf_i %>% 
             ## group at the minute level
             group_by(date_vec, time_vec) %>%
             ## obtain mean and number of not imputed minutes
             summarize("acceleration" = mean(acceleration,na.rm=TRUE),
                      "imputed" = sum(1-imputed,na.rm=TRUE),
                      "eid" = eid[1]) %>%
             ungroup()

  ## transform to wide format for accelration and imputation separately using tidyr::spread
  accel_mat_i  <- adf_i %>% select(-imputed) %>% spread(time_vec,acceleration)
  impute_mat_i <- adf_i %>% select(-acceleration) %>% spread(time_vec,imputed)
  
  
  ## assign subject i's data to the corresponding names 
  assign(accel_mat_nms[i], accel_mat_i)
  assign(impute_mat_nms[i], impute_mat_i)
 
  ## clear up the workspace a bit
  rm(accel_i, interval_i, raw_date_i, date_i, 
     date_time_vec,date_time_vec_chr,time_vec,date_vec,
     adf_i,accel_mat_i, impute_mat_i)

}

## combine the data
accel_mat_nms  <- ls()[grepl("accel_mat_[0-9]+", ls())]
impute_mat_nms <- ls()[grepl("impute_mat_[0-9]+", ls())]

eval(parse(text=paste0("accel_mat <- bind_rows(", paste0(accel_mat_nms, collapse=","),")")))
eval(parse(text=paste0("impute_mat <- bind_rows(", paste0(impute_mat_nms, collapse=","),")")))
```




# Analysis pipeline using the transformed data

Analyzing the transformed data involves first identifying "good" accelerometry data, calculating features of interest, and then associating those features with 
one or more variables of interest. Below we present our approach to identifying good accelerometry data in the UK Biobank using the processed 5-second level data which we aggregated at the minute level above, and then 


## Subsetting to "good" accelerometry data

Identifying "good" accelerometry data typically has two major components. 
The first component is to determine whether the data are reliable (i.e. was the device appropriately calibrated). 
Once the a particular devices' data has been deemed "reliable", one then identifies participant complinace with weartime protocols. 
Here, we opt for a conservative definition of "reliable" data based on variables derived from the UK Biobank team. 


```{r identify_good_accel_data, echo=TRUE, message=FALSE}


```


## Exploratory plots of daily activity profiles


```{r plot_PA_profiles, echo=TRUE, message=FALSE}


```


## Calculating scalar summaries of activity profiles


```{r calculate_scalar_summaries, echo=TRUE, message=FALSE}


```



# References
